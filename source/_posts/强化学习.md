---
title: 强化学习基础
tag:
    - ML

categories: ML
mathjax: true
---


强化学习是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。强化学习和标准的监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡

## 预备知识

1. 强化学习的基础概念包括主体，环境，状态，动作，奖励

- 主体(agent) 是动作的行使者
- 状态(state) 是主体的处境
- 动作(action) 主体的动作，主体需要在一些列潜在的动作中进行选择
- 奖励(reward) 主体行使动作的反馈。主体向环境发出以动作为形式的输出，环境返回主体的新状态以及奖励。
- 策略：一个”动态-动作”的集合

2. 马尔可夫性：系统的下个状态只与当前状态信息有关，而与更早的信息无关

---  | 不考虑动作 | 考虑动作
---|---|---
状态完全可见 | 马尔可夫链(MC)  |  马尔可夫决策过程
状态不完全可见  | 隐马尔科夫模型(HMM)  |  不完全可观察马尔可夫决策过程(POMDP)

3. 在强化学习中，机器需要做的是通过在环境中不断地尝试而学得一个“策略” $\pi$，根据这个策略，在状态 $x$ 下就能得知想要执行的动作 $a = \pi(x)$。策略的优劣取决于长期执行这一策略后得到的累积奖励 $V$

4. 强化学习分为有模型的学习和无模型的学习

## 有模型的学习
在模型已知时，对于任意的策略 $\pi$ 可以估计出该策略带来的期望累积奖励。**令 $V^\pi(x)$ 表示从状态 $x$ 出发，使用策略 $\pi$ 所带来的累积奖励； $Q^\pi(x, a)$ 表示从状态 $x$ 出发，执行动作 $a$ 后带来的累积奖励**。这里 $V$ 称为“状态值函数”， $Q$ 称为“状态-动作值函数”。根据定义，有：
$$
\left\{
\begin{array}{c}
V_T^\pi(x) = E_\pi[\frac{1}{T}\sum_{t=1}^{T}{r_t} | x_0 = x] \\
V_\gamma^\pi(x) = E_\pi[\sum_{t=0}^{\infty}{\gamma^t * r_{t+1}} | x_0 = x]
\end{array}
\right.
\tag{1}
$$
第一个式子指 $T$ 步累积奖励，表示状态值函数与接下来的 $T$ 步有关。第二个式子指 $\gamma$ 折扣奖励，表示状态值函数与接下来的所有步都有关。 $x_0 = x$ 表示初始状态为 $x$
而状态-动作值函数：
$$
\left\{
\begin{array}{c}
Q_T^\pi(x,a) = E_\pi[\frac{1}{T}\sum_{t=1}^{T}{r_t} | x_0 = x, a_0 = a] \\
Q_\gamma^\pi(x,a) = E_\pi[\sum_{t=0}^{\infty}{\gamma^t * r_{t+1}} | x_0 = x, a_0 = a]
\end{array}
\right.
$$
本文后续均以 $T$ 步累积奖励为例子

模型已知时，对状态值函数进行全概率展开：
$$
\begin{align}
V_T^\pi(x) &= E_\pi[\frac{1}{T}\sum_{t=1}^{T}{r_t} | x_0 = x] \\
&= E_\pi[\frac{1}{T}r_1 + \frac{1}{T}\sum_{t=2}^{T}{r_t} | x_0 = x] \\
&= E_\pi[\frac{1}{T}r_1 + \frac{T-1}{T}\frac{1}{T-1}\sum_{t=1}^{T-1}{r_t} | x_0 = x] \\
&= \sum\limits_{a\in A}{\pi(x,a)}\sum\limits_{x'\in X}{P_{x\to x'}^{a}(\frac{1}{T}R_{x\to x'}^{a} + \frac{T-1}{T}E_\pi[\frac{1}{T-1}\sum_{t=1}{T-1}t_t | x_0 = x'])}  \\
&= \sum\limits_{a\in A}{\pi(x,a)}\sum\limits_{x'\in X}{P_{x\to x'}^{a}(\frac{1}{T}R_{x\to x'}^{a} + \frac{T-1}{T}V_{T-1}^{\pi}(x'))}
\end{align}
$$
其中 $\pi(x, a)$ 代表随机策略 $\pi$ 在状态 $x$ 下进行动作 $a$ 的概率； $P$ 代表在状态 $x$ 下进行动作 $a$ 进入状态 $x'$ 的概率; $R$ 代表在状态 $x$ 下进行动作 $a$ 进入状态 $x'$ 的奖励。需要注意的是，**因为P和R已知才可以在第四行全概率展开!!!** 此时：
$$Q_T^\pi(x, a) = \sum\limits_{x'\in X}{P_{x\to x'}^{a}(\frac{1}{T}R_{x\to x'}^{a} + \frac{T-1}{T}V_{T-1}^{\pi}(x'))}$$

这样就可以得到一个基于 $T$ 步累积奖励的策略评估算法($\gamma$累积奖励同理，循环停止条件为设置值函数更新量小于阈值则停止):
![策略评估](http://cjxuyshare.me/pic/celuepinggu.png)

对于我们需要的最优的策略 $\pi^*$，对于任意状态 $x$ 对应的动作 $a$ 是固定的，即 $\sum\limits_{a'\in A}{\pi^*(x, a')} = \pi^*(x, a) = 1$，因此，最优策略对应的状态值函数为：
$$V_T^*(x) = \max\limits_{a\in A} \sum\limits_{x'\in X}{P_{x\to x'}^{a}(\frac{1}{T}R_{x\to x'}^{a} + \frac{T-1}{T}V_{T-1}^{\pi}(x'))} \tag{2}$$
也即 $V_T^*(x) = \max\limits_{a\in A} Q_T^*(x, a)$，代入$2$：
$$Q_T^*(x, a) = \sum\limits_{x'\in X}{P_{x\to x'}^{a}(\frac{1}{T}R_{x\to x'}^{a} + \frac{T-1}{T}\max\limits_{a'\in A} Q_{T-1}^*(x', a'))} \tag{3}$$

$2,3$合称为最优bellman等式，它揭示了非最优策略的改进方式：讲策略选择的动作改变为当前状态下的最优动作。这样我们就知道了如何评估一个策略以及如何改进一个非最优策略，将两者结合起来就可以得到求解最优策略的方法：

1. **策略迭代：从一个初始策略(通常是随机策略)出发，先进行策略评估，然后改进策略，一直迭代直到策略不再改变为止**
![策略迭代](http://cjxuyshare.me/pic/策略迭代.png)
2. **值迭代：根据$2$，策略的改进与状态值函数的改进是一致的，求得最优的状态值函数，对应的策略就是最优的策略**
![值迭代](http://cjxuyshare.me/pic/值迭代.png)

## 免模型学习
对于很多任务来说，环境的转移概率和奖励函数很难得知，因此无法对状态值函数进行全概率展开，也就无从进行策略评估，在未知环境下，主体只能从一个特定状态或状态集开始探索环境，然后在探索过程中逐渐发现各种状态并估计各状态-动作值函数。。因此我们选择一种替代方法：**使用策略多次采样，然后求取平均累积奖励来作为期望奖累积奖励的近似。这称为蒙特卡罗强化学习算法**

在模型未知情况下，我们从初始状态出发，使用某确定性策略进行采样，执行T步策略并获得轨迹 $$<x_0,a_0,r_1,x_1,a_1,r_2,...,x_{T-1},a_{T-1},r_T,x_T>$$多次采样得到多条轨迹后，将每个状态-动作对的累积奖励进行平均，得到状态-动作值函数的估计。
为了得到多条轨迹，我们将确定性的策略称为“原始策略”，在原始策略上使用 $\varepsilon$-贪心法：
$$
\pi^\varepsilon(x) =
\left\{
\begin{array}{c}
\pi(x), 以概率 1-\varepsilon \\
A中动作均匀概率选取，以概率\varepsilon
\end{array}
\right.
$$
在 $\varepsilon$-贪心策略中，上一轮计算得到的最优动作被选取的概率为 $1-\varepsilon + \frac{\varepsilon}{A}$，非最优动作被选取的概率为 $\frac{\varepsilon}{A}$

1. 同策略蒙特卡罗强化学习算法
![same_mtkl](http://cjxuyshare.me/pic/samemengtekaluo.png)

注意，此算法最终获得的是 $\varepsilon$-贪心策略。然而，引入 $\varepsilon$-贪心策略目的是为了评估而不是最终的使用。考虑使用 $\varepsilon$-贪心策略进行评估，而原始策略作为结果。
对于函数 $f$ 在概率分布 $p$ 下的期望：
$$E[f] = \int p(x)f(x)dx$$
可以通过从概率分布 $p$ 上的采样来估计 $f$ 的期望：
$$E[f] = \frac{1}{m}\sum_{i=1}^{m} f(x_i)$$
如果引入另一个分布 $q$，则函数 $f$ 在概率分布 $p$ 下的期望等价于：
$$E[f] = \int q(x)\frac{p(x)}{q(x)}f(x)dx$$
上式可以看做 $\frac{p(x)}{q(x)}f(x)$ 在概率分布 $q$ 下的期望，因此可以通过在 $q$ 下的采样来表示：
$$E[f] = \frac{1}{m}\sum_{i=1}^{m} \frac{p(x')}{q(x')}f(x')$$
回到我们的问题上来，**使用策略 $\pi$ 的采样轨迹来评估策略 $\pi$ 实际上就是对累积奖赏估计期望**
$$Q(x,a) = \frac{1}{m} \sum_{i=1}^m R_i$$
其中 $R_i$ 表示第i条轨迹从状态x到结束的累积奖赏，如果用策略 $\pi'$ 来评估 $\pi$，则仅需对累积奖赏加权：
$$Q(x,a) = \frac{1}{m} \sum_{i=1}^m \frac{P_i^\pi}{P_i^{\pi'}}R_i$$
其中， $P, Q$ 分别表示两个策略产生第i条轨迹的概率，则：
$$P^\pi = \prod_{i=0}^{T-1} \pi(x_i,a_i)P_{x_i \to x_{i+1}}^{a_i}$$
此处只用到比值：
$$\frac{P^\pi}{P^{\pi'}} = \prod_{i=0}^{T-1} \frac{\pi(x_i,a_i)}{\pi'(x_i,a_i)}$$
对于确定性策略 $\pi$ 和其 $\varepsilon$-贪心策略 $\pi'$，$\pi(x_i,a_i)$ 对于 $a_i = \pi(x_i)$ 始终为1，而 $\pi'(x_i, a_i)$ 为 $1-\varepsilon + \frac{\varepsilon}{A}$，或 $\frac{\varepsilon}{A}$

2. 异策略蒙特卡罗强化学习算法
![dif_mtkl](http://cjxuyshare.me/pic/difmengtekaluo.png)
其中第6行的公式有点问题，应该改为 $R = \frac{1}{T-t} (\sum_{i=t+1}{T} r_i)\prod_{i=t+1}^{T-1} \frac{if(a_i = \pi(x_i))}{p_i}$

蒙特卡罗强化学习算法的本质是通过多次尝试采样轨迹求平均来作为累积奖励的期望，求平均的过程是“批量式”的，这个过程可以改进为“增量式”：
$$Q_{t+1}^\pi(x, a) = \frac{tQ_t^\pi + R_{t+1}}{t+1} = Q_t^\pi(x, a) + \frac{1}{t+1}(R_{t+1} - Q_t^\pi(x, a))$$
这里
$$R_{t+1} = R_{x \to x'}^{a} + \gamma \max Q_t^\pi(x',a') = r_{t+1} + \gamma Q_t^\pi(x',a')$$
是从当前步起到结束的累积奖励。令系数 $\frac{1}{t+1} = \alpha$ 不会影响Q是累积奖赏的这一性质，更新步长 $\alpha$ 越大，则越靠后的累计奖励越重要。
最终得到：
$$Q_{t+1}^\pi(x, a) = \frac{tQ_t^\pi + R_{t+1}}{t+1} = Q_t^\pi(x, a) + \alpha(R_{t+1} - Q_t^\pi(x, a)) \tag{4}$$

观察$4$，发现Q的更新就是朝着 $R$ 更新的，直到 $Q = R_{t+1}$ 停止，这样就可以看成是一个学习过程，目标值是 $R$，预测值是 $Q$。这也是Q-learn名称的由来。DQN就是吧这个学习过程用神经网络来完成，本质是一样的！

综上，得到Q-learn算法：
![q-learn](http://cjxuyshare.me/pic/q-learn.png)



--------
参考资料：

* 周志华：《机器学习》
